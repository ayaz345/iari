import logging
import re
from io import BytesIO
from typing import List

import requests
from PyPDF2 import PdfReader  # type: ignore
from PyPDF2.errors import PdfReadError
from pydantic import BaseModel

from src.models.api.job.check_url_job import UrlJob
from src.models.exceptions import MissingInformationError

logger = logging.getLogger(__name__)


class PyPdf2Handler(BaseModel):
    job: UrlJob
    content: bytes = b""
    links: List[str] = []
    error: bool = False

    def __download_pdf__(self):
        """Download PDF file from URL"""
        if not self.content:
            response = requests.get(self.job.url, timeout=self.job.timeout)
            if response.content:
                self.content = response.content
            else:
                logger.warning("Got no pdf content from requests")

    def __extract_links__(self) -> None:
        """Extract all links from PDF file"""
        if not self.content:
            raise MissingInformationError()
        links = []
        with BytesIO(self.content) as pdf_file:
            try:
                pdf_reader = PdfReader(pdf_file)
                for page in pdf_reader.pages:
                    text = page.extract_text()
                    # provided by chatgpt
                    regex = r"https?://(?:[a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}(?:/[^\s]*)?"
                    url = re.findall(regex, text)
                    links.extend(url)
                self.links = links
            except PdfReadError:
                logger.error("Not a valid PDF")
                self.error = True

    def download_and_extract(self):
        self.__download_pdf__()
        self.__extract_links__()
        if not self.error:
            self.__discard_invalid_urls__()
            self.__clean_spaces__()

    def get_dict(self):
        return dict(links=self.links)

    def __clean_spaces__(self):
        """Some links have spaces in them when returned from pypdf2 so we fix that"""
        links = []
        for link in self.links:
            # this was generated by chatgpt
            clean_link = (
                link.replace("\u0020", "")
                .replace("\u0009", "")
                .replace("\u000A", "")
                .replace("\u000B", "")
                .replace("\u000C", "")
                .replace("\u000D", "")
                .replace("\u0085", "")
                .replace("\u1680", "")
                .replace("\u180E", "")
                .replace("\u2000", "")
                .replace("\u2001", "")
                .replace("\u2002", "")
                .replace("\u2003", "")
                .replace("\u2004", "")
                .replace("\u2005", "")
                .replace("\u2006", "")
                .replace("\u2007", "")
                .replace("\u2008", "")
                .replace("\u2009", "")
                .replace("\u200A", "")
                .replace("\u2028", "")
                .replace("\u2029", "")
                .replace("\u202F", "")
                .replace("\u205F", "")
                .replace("\u3000", "")
            )
            # logger.debug(f"output: {clean_link}")
            links.append(clean_link)
        self.links = links

    def __discard_invalid_urls__(self):
        """This should discard any url with "www" and without 2 dots and a tld
        e.g. https://www.science"""
        links = []
        # generated by chatgpt and edited by hand by Dennis
        regex = r".*\.[^\.]+\.[a-zA-Z]{2,}.*"
        for link in self.links:
            if "www" in link and not re.search(regex, link):
                # had www but not 2 dots and a tld
                logger.info(f"discarded url: {link}")
            else:
                links.append(link)
        self.links = links
